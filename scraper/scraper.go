package main

import (
	"encoding/json"
	"fmt"
	"log"
	"math/rand"
	"os"
	"runtime"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/gocolly/colly"
)

// Variables de versioning inject√©es lors du build
// Ces valeurs sont remplac√©es par les flags de compilation lors du build Docker
var (
	version   = "dev"     // Version de l'application
	gitCommit = "unknown" // Hash du commit Git
	buildTime = "unknown" // Timestamp de compilation
)

// BuildInfo supprim√© - non utilis√© apr√®s r√©duction des logs

// Recipe repr√©sente une recette compl√®te avec tous ses d√©tails
type Recipe struct {
	Name         string        `json:"name"`         // Nom de la recette
	Page         string        `json:"page"`         // URL de la page de la recette
	Image        string        `json:"image"`        // URL de l'image de la recette
	Ingredients  []Ingredient  `json:"ingredients"`  // Liste des ingr√©dients
	Instructions []Instruction `json:"instructions"` // Liste des instructions
}

// Ingredient repr√©sente un ingr√©dient avec sa quantit√© et son unit√©
type Ingredient struct {
	Quantity string `json:"quantity"` // Quantit√© (ex: "2", "1/2")
	Unit     string `json:"unit"`     // Unit√© (ex: "cups", "tablespoons")
}

// Instruction repr√©sente une √©tape de la recette
type Instruction struct {
	Number      string `json:"number"`      // Num√©ro de l'√©tape (ex: "1", "2")
	Description string `json:"description"` // Description de l'√©tape
}

// RecipeData contient les informations de base d'une recette avant le scraping d√©taill√©
// Utilis√© pour passer les donn√©es entre les goroutines
type RecipeData struct {
	URL   string // URL de la page de la recette
	Title string // Titre de la recette
	Image string // URL de l'image de la recette
}

// ScrapingStats contient toutes les statistiques de performance du scraper
// Thread-safe gr√¢ce au Mutex pour les acc√®s concurrents
type ScrapingStats struct {
	// Compteurs de requ√™tes HTTP
	TotalRequests    int64 `json:"total_requests"`     // Total des requ√™tes HTTP
	MainPageRequests int64 `json:"main_page_requests"` // Requ√™tes vers les pages de cat√©gories
	RecipeRequests   int64 `json:"recipe_requests"`    // Requ√™tes vers les pages de recettes

	// Compteurs de recettes
	RecipesFound     int64 `json:"recipes_found"`     // Nombre de recettes d√©couvertes
	RecipesCompleted int64 `json:"recipes_completed"` // Nombre de recettes trait√©es avec succ√®s
	RecipesFailed    int64 `json:"recipes_failed"`    // Nombre de recettes en √©chec

	// M√©triques de performance temporelles
	StartTime         time.Time     `json:"start_time"`          // Heure de d√©but du scraping
	EndTime           time.Time     `json:"end_time"`            // Heure de fin du scraping
	TotalDuration     time.Duration `json:"total_duration"`      // Dur√©e totale du scraping
	RequestsPerSecond float64       `json:"requests_per_second"` // Requ√™tes par seconde
	RecipesPerSecond  float64       `json:"recipes_per_second"`  // Recettes par seconde

	// Configuration des workers
	MaxWorkers    int   `json:"max_workers"`    // Nombre maximum de workers
	ActiveWorkers int64 `json:"active_workers"` // Nombre de workers actifs

	// Statistiques d√©taill√©es par worker
	WorkerStats map[int]WorkerStats `json:"worker_stats"` // Map des stats par worker

	Mutex sync.RWMutex // Mutex pour la s√©curit√© des acc√®s concurrents
}

// WorkerStats contient les statistiques d'un worker individuel
type WorkerStats struct {
	WorkerID         int           `json:"worker_id"`         // ID unique du worker
	RequestsHandled  int64         `json:"requests_handled"`  // Nombre de requ√™tes trait√©es
	RecipesProcessed int64         `json:"recipes_processed"` // Nombre de recettes trait√©es
	StartTime        time.Time     `json:"start_time"`        // Heure de d√©marrage du worker
	EndTime          time.Time     `json:"end_time"`          // Heure de fin du worker
	Duration         time.Duration `json:"duration"`          // Dur√©e totale d'activit√©
}

// NewScrapingStats cr√©e une nouvelle instance de ScrapingStats
// maxWorkers: nombre maximum de workers qui seront utilis√©s
func NewScrapingStats(maxWorkers int) *ScrapingStats {
	return &ScrapingStats{
		StartTime:   time.Now(),                // Initialiser avec l'heure actuelle
		MaxWorkers:  maxWorkers,                // Stocker le nombre max de workers
		WorkerStats: make(map[int]WorkerStats), // Initialiser la map des stats par worker
	}
}

// IncrementMainPageRequest incr√©mente le compteur de requ√™tes vers les pages principales
// Thread-safe gr√¢ce au mutex
func (s *ScrapingStats) IncrementMainPageRequest() {
	s.Mutex.Lock()
	defer s.Mutex.Unlock()
	s.TotalRequests++    // Incr√©menter le total des requ√™tes
	s.MainPageRequests++ // Incr√©menter les requ√™tes vers les pages principales
}

// IncrementRecipeRequest incr√©mente le compteur de requ√™tes vers les pages de recettes
// Thread-safe gr√¢ce au mutex
func (s *ScrapingStats) IncrementRecipeRequest() {
	s.Mutex.Lock()
	defer s.Mutex.Unlock()
	s.TotalRequests++  // Incr√©menter le total des requ√™tes
	s.RecipeRequests++ // Incr√©menter les requ√™tes vers les recettes
}

// IncrementRecipesFound incr√©mente le compteur de recettes d√©couvertes
// Thread-safe gr√¢ce au mutex
func (s *ScrapingStats) IncrementRecipesFound() {
	s.Mutex.Lock()
	defer s.Mutex.Unlock()
	s.RecipesFound++ // Incr√©menter le nombre de recettes trouv√©es
}

// IncrementRecipesCompleted incr√©mente le compteur de recettes trait√©es avec succ√®s
// Thread-safe gr√¢ce au mutex
func (s *ScrapingStats) IncrementRecipesCompleted() {
	s.Mutex.Lock()
	defer s.Mutex.Unlock()
	s.RecipesCompleted++ // Incr√©menter le nombre de recettes compl√©t√©es
}

// IncrementRecipesFailed incr√©mente le compteur de recettes en √©chec
// Thread-safe gr√¢ce au mutex
func (s *ScrapingStats) IncrementRecipesFailed() {
	s.Mutex.Lock()
	defer s.Mutex.Unlock()
	s.RecipesFailed++ // Incr√©menter le nombre de recettes √©chou√©es
}

func (s *ScrapingStats) UpdateWorkerStats(workerID int, requests, recipes int64) {
	s.Mutex.Lock()
	defer s.Mutex.Unlock()

	if worker, exists := s.WorkerStats[workerID]; exists {
		worker.RequestsHandled += requests
		worker.RecipesProcessed += recipes
		worker.EndTime = time.Now()
		worker.Duration = worker.EndTime.Sub(worker.StartTime)
		s.WorkerStats[workerID] = worker
	} else {
		s.WorkerStats[workerID] = WorkerStats{
			WorkerID:         workerID,
			RequestsHandled:  requests,
			RecipesProcessed: recipes,
			StartTime:        time.Now(),
			EndTime:          time.Now(),
			Duration:         0,
		}
	}
}

func (s *ScrapingStats) GetTotalRequests() int64 {
	s.Mutex.RLock()
	defer s.Mutex.RUnlock()
	return s.TotalRequests
}

func (s *ScrapingStats) CalculateFinalStats() {
	s.Mutex.Lock()
	defer s.Mutex.Unlock()

	s.EndTime = time.Now()
	s.TotalDuration = s.EndTime.Sub(s.StartTime)

	if s.TotalDuration.Seconds() > 0 {
		s.RequestsPerSecond = float64(s.TotalRequests) / s.TotalDuration.Seconds()
		s.RecipesPerSecond = float64(s.RecipesCompleted) / s.TotalDuration.Seconds()
	}
}

func (s *ScrapingStats) GetDetailedStats() ScrapingStats {
	s.Mutex.RLock()
	defer s.Mutex.RUnlock()

	// Cr√©er une copie sans le mutex
	return ScrapingStats{
		TotalRequests:     s.TotalRequests,
		MainPageRequests:  s.MainPageRequests,
		RecipeRequests:    s.RecipeRequests,
		RecipesFound:      s.RecipesFound,
		RecipesCompleted:  s.RecipesCompleted,
		RecipesFailed:     s.RecipesFailed,
		StartTime:         s.StartTime,
		EndTime:           s.EndTime,
		TotalDuration:     s.TotalDuration,
		RequestsPerSecond: s.RequestsPerSecond,
		RecipesPerSecond:  s.RecipesPerSecond,
		MaxWorkers:        s.MaxWorkers,
		ActiveWorkers:     s.ActiveWorkers,
		WorkerStats:       s.WorkerStats,
	}
}

// getPhysicalCores d√©tecte le vrai nombre de c≈ìurs physiques
func getPhysicalCores() int {
	// M√©thode 1: Lire /proc/cpuinfo sur Linux
	if runtime.GOOS == "linux" {
		if cores := detectPhysicalCoresFromProc(); cores > 0 {
			return cores
		}
	}

	// M√©thode 2: Estimation intelligente bas√©e sur les patterns courants
	numLogicalCPU := runtime.NumCPU()

	// Patterns courants d'hyperthreading
	switch {
	case numLogicalCPU == 1:
		return 1
	case numLogicalCPU == 2:
		return 2 // Probablement 2 c≈ìurs sans HT
	case numLogicalCPU == 4:
		return 2 // Probablement 2 c≈ìurs avec HT
	case numLogicalCPU == 6:
		return 6 // Probablement 6 c≈ìurs sans HT
	case numLogicalCPU == 8:
		return 4 // Probablement 4 c≈ìurs avec HT
	case numLogicalCPU == 12:
		return 6 // Probablement 6 c≈ìurs avec HT
	case numLogicalCPU == 16:
		return 8 // Probablement 8 c≈ìurs avec HT
	case numLogicalCPU == 24:
		return 12 // Probablement 12 c≈ìurs avec HT
	case numLogicalCPU == 32:
		return 16 // Probablement 16 c≈ìurs avec HT
	case numLogicalCPU%2 == 0:
		// Si pair, essayer de diviser par 2 (hyperthreading probable)
		estimated := numLogicalCPU / 2
		if estimated >= 1 {
			return estimated
		}
	}

	// Fallback: utiliser le nombre logique
	return numLogicalCPU
}

// detectPhysicalCoresFromProc lit /proc/cpuinfo pour d√©tecter les vrais c≈ìurs physiques
func detectPhysicalCoresFromProc() int {
	// Cette fonction serait impl√©ment√©e pour lire /proc/cpuinfo
	// et compter les vrais c≈ìurs physiques
	// Pour l'instant, on retourne 0 pour utiliser la m√©thode de fallback
	return 0
}

// calculateAdaptiveRatio calcule le ratio optimal bas√© sur le nombre de c≈ìurs
func calculateAdaptiveRatio(numCores int) int {
	switch {
	case numCores <= 2:
		return 3 // Plus de workers sur machines faibles pour compenser
	case numCores <= 4:
		return 2 // Ratio standard pour machines moyennes
	case numCores <= 8:
		return 2 // Ratio standard pour machines puissantes
	case numCores <= 16:
		return 1 // Moins de workers sur tr√®s grosses machines (√©viter la surcharge)
	default:
		return 1 // Ratio conservateur pour machines extr√™mes
	}
}

// calculateOptimalWorkers calcule le nombre optimal de workers bas√© sur les ressources CPU
// minWorkers: nombre minimum de workers (par d√©faut 1)
// maxWorkers: nombre maximum de workers (par d√©faut 50)
func calculateOptimalWorkers(minWorkers, maxWorkers int) int {
	// D√©tecter le vrai nombre de c≈ìurs physiques
	numPhysicalCores := getPhysicalCores()

	// Calculer le ratio adaptatif bas√© sur le nombre de c≈ìurs
	adaptiveRatio := calculateAdaptiveRatio(numPhysicalCores)

	optimalWorkers := numPhysicalCores * adaptiveRatio

	// Appliquer les limites
	if optimalWorkers < minWorkers {
		optimalWorkers = minWorkers
	}
	if optimalWorkers > maxWorkers {
		optimalWorkers = maxWorkers
	}

	return optimalWorkers
}

// printVersionInfo affiche les informations de version (simplifi√©)
func printVersionInfo() {
	// Logs de version supprim√©s pour r√©duire la verbosit√©
}

// getBuildInfo supprim√© - non utilis√© apr√®s r√©duction des logs

// userAgents contient une liste de User-Agents r√©alistes pour simuler diff√©rents navigateurs
var userAgents = []string{
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
	"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
}

var userAgentMutex sync.Mutex
var userAgentIndex = 0

// getRandomUserAgent retourne un User-Agent al√©atoire de la liste
func getRandomUserAgent() string {
	userAgentMutex.Lock()
	defer userAgentMutex.Unlock()

	// Utiliser un index rotatif pour distribuer les User-Agents
	userAgentIndex = (userAgentIndex + 1) % len(userAgents)
	return userAgents[userAgentIndex]
}

// configureRealisticHeaders configure les headers HTTP pour simuler un navigateur r√©el
func configureRealisticHeaders(r *colly.Request) {
	// User-Agent r√©aliste
	r.Headers.Set("User-Agent", getRandomUserAgent())

	// Headers standards d'un navigateur moderne
	r.Headers.Set("Accept", "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7")
	r.Headers.Set("Accept-Language", "en-US,en;q=0.9,fr;q=0.8")
	r.Headers.Set("Accept-Encoding", "gzip, deflate, br, zstd")
	r.Headers.Set("DNT", "1")
	r.Headers.Set("Connection", "keep-alive")
	r.Headers.Set("Upgrade-Insecure-Requests", "1")
	r.Headers.Set("Sec-Fetch-Dest", "document")
	r.Headers.Set("Sec-Fetch-Mode", "navigate")
	r.Headers.Set("Sec-Fetch-Site", "none")
	r.Headers.Set("Sec-Fetch-User", "?1")
	r.Headers.Set("Cache-Control", "max-age=0")

	// Headers sec-ch-ua pour simuler un navigateur moderne (Chrome/Edge)
	r.Headers.Set("sec-ch-ua", `"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"`)
	r.Headers.Set("sec-ch-ua-mobile", "?0")
	r.Headers.Set("sec-ch-ua-platform", `"Windows"`)

	// Ajouter un Referer r√©aliste
	if r.URL != nil && r.URL.Host != "" {
		// Pour la premi√®re visite, utiliser Google comme referer
		if !strings.Contains(r.URL.String(), "allrecipes.com") || r.URL.Path == "/" {
			r.Headers.Set("Referer", "https://www.google.com/")
		} else {
			// Pour les pages internes, utiliser le domaine comme referer
			r.Headers.Set("Referer", "https://www.allrecipes.com/")
		}
	} else {
		// Referer par d√©faut pour la premi√®re visite
		r.Headers.Set("Referer", "https://www.google.com/")
	}
}

// getRandomDelay retourne un d√©lai al√©atoire entre min et max millisecondes
func getRandomDelay(minMs, maxMs int) time.Duration {
	if maxMs <= minMs {
		return time.Duration(minMs) * time.Millisecond
	}
	delay := minMs + rand.Intn(maxMs-minMs+1)
	return time.Duration(delay) * time.Millisecond
}

// createMainCollector cr√©e et configure le collecteur principal pour les pages de cat√©gories
// Ce collecteur visite les pages de listes de recettes et extrait les URLs des recettes individuelles
func createMainCollector(stats *ScrapingStats, recipeURLs chan<- RecipeData) *colly.Collector {
	collector := colly.NewCollector()

	// Configuration des limites pour √™tre respectueux du serveur
	// D√©lais augment√©s et parall√©lisme r√©duit pour √©viter la d√©tection
	collector.Limit(&colly.LimitRule{
		DomainGlob:  "*",                    // Appliquer √† tous les domaines
		Parallelism: 3,                      // R√©duit √† 3 requ√™tes simultan√©es
		Delay:       500 * time.Millisecond, // D√©lai de base de 500ms entre les requ√™tes
		RandomDelay: 1 * time.Second,        // D√©lai al√©atoire jusqu'√† 1 seconde (fonctionnalit√© native Colly)
	})

	// Handler appel√© avant chaque requ√™te HTTP
	collector.OnRequest(func(r *colly.Request) {
		// Configurer les headers r√©alistes pour √©viter la d√©tection
		configureRealisticHeaders(r)

		// Les d√©lais al√©atoires sont g√©r√©s automatiquement par Colly via RandomDelay dans LimitRule
		stats.IncrementMainPageRequest() // Incr√©menter le compteur de requ√™tes
		// Log de requ√™te supprim√© pour r√©duire la verbosit√©
	})

	// G√©rer les erreurs HTTP (403, 429, etc.)
	collector.OnError(func(r *colly.Response, err error) {
		statusCode := r.StatusCode
		if statusCode == 403 || statusCode == 429 {
			log.Printf("‚ö†Ô∏è  Erreur %d d√©tect√©e pour %s: %v\n", statusCode, r.Request.URL, err)
			log.Printf("üîÑ Attente prolong√©e avant retry (10-20s)...\n")
			// Attendre beaucoup plus longtemps en cas d'erreur (10-20 secondes)
			time.Sleep(getRandomDelay(10000, 20000))
		} else {
			log.Printf("‚ùå Erreur HTTP %d pour %s: %v\n", statusCode, r.Request.URL, err)
		}
	})

	// Handler appel√© pour chaque √©l√©ment HTML correspondant au s√©lecteur CSS
	// Ce s√©lecteur cible les cartes de recettes sur AllRecipes
	collector.OnHTML("div.mntl-taxonomysc-article-list-group .mntl-card", func(e *colly.HTMLElement) {
		// Extraire l'URL, le titre et l'image de la recette
		page := e.Request.AbsoluteURL(e.Attr("href")) // URL de la page de la recette
		title := e.ChildText("span.card__title-text") // Titre de la recette
		image := e.ChildAttr("img", "data-src")       // URL de l'image

		// V√©rifier que nous avons les donn√©es essentielles
		if page != "" && title != "" {
			stats.IncrementRecipesFound() // Incr√©menter le compteur de recettes trouv√©es

			// Cr√©er l'objet RecipeData avec les informations extraites
			recipeData := RecipeData{
				URL:   page,
				Title: title,
				Image: image,
			}

			// Envoyer la recette dans le channel (non-bloquant)
			select {
			case recipeURLs <- recipeData:
				// Log supprim√© pour r√©duire la verbosit√© (trop de logs)
			default:
				log.Printf("‚ö†Ô∏è  Channel plein, recette ignor√©e: '%s'\n", title)
			}
		}
	})

	return collector
}

// createMainCollectorWithPagination cr√©e un collecteur avec support de la pagination
func createMainCollectorWithPagination(stats *ScrapingStats, recipeURLs chan<- RecipeData, maxPages int) *colly.Collector {
	collector := colly.NewCollector()

	// Configuration des limites avec d√©lais plus longs pour √©viter la d√©tection
	// Parall√©lisme r√©duit √† 1 pour √©viter la d√©tection anti-bot
	collector.Limit(&colly.LimitRule{
		DomainGlob:  "*",
		Parallelism: 1,               // R√©duit √† 1 requ√™te √† la fois pour √©viter la d√©tection
		Delay:       2 * time.Second, // D√©lai de base augment√© √† 2 secondes
		RandomDelay: 2 * time.Second, // D√©lai al√©atoire jusqu'√† 2 secondes (fonctionnalit√© native Colly)
	})

	// Map pour suivre les pages visit√©es par cat√©gorie
	visitedPages := make(map[string]int)
	var mutex sync.Mutex

	collector.OnRequest(func(r *colly.Request) {
		// Configurer les headers r√©alistes pour √©viter la d√©tection
		configureRealisticHeaders(r)

		// Les d√©lais al√©atoires sont g√©r√©s automatiquement par Colly via RandomDelay dans LimitRule
		stats.IncrementMainPageRequest()
		// Log de requ√™te supprim√© pour r√©duire la verbosit√©
	})

	// G√©rer les erreurs HTTP (403, 429, etc.)
	collector.OnError(func(r *colly.Response, err error) {
		statusCode := r.StatusCode
		if statusCode == 403 || statusCode == 429 {
			log.Printf("‚ö†Ô∏è  Erreur %d d√©tect√©e pour %s: %v\n", statusCode, r.Request.URL, err)
			log.Printf("üîÑ Attente prolong√©e avant retry (10-20s)...\n")
			// Attendre beaucoup plus longtemps en cas d'erreur (10-20 secondes)
			time.Sleep(getRandomDelay(10000, 20000))
		} else {
			log.Printf("‚ùå Erreur HTTP %d pour %s: %v\n", statusCode, r.Request.URL, err)
		}
	})

	// G√©rer les recettes sur la page actuelle
	collector.OnHTML("div.mntl-taxonomysc-article-list-group .mntl-card", func(e *colly.HTMLElement) {
		page := e.Request.AbsoluteURL(e.Attr("href"))
		title := e.ChildText("span.card__title-text")
		image := e.ChildAttr("img", "data-src")

		if page != "" && title != "" {
			stats.IncrementRecipesFound()
			recipeData := RecipeData{
				URL:   page,
				Title: title,
				Image: image,
			}

			select {
			case recipeURLs <- recipeData:
				// Log supprim√© pour r√©duire la verbosit√© (trop de logs)
			default:
				log.Printf("‚ö†Ô∏è  Channel plein, recette ignor√©e: '%s'\n", title)
			}
		}
	})

	// G√©rer la pagination
	collector.OnHTML("a[data-testid='pagination-next']", func(e *colly.HTMLElement) {
		nextPageURL := e.Request.AbsoluteURL(e.Attr("href"))
		if nextPageURL == "" {
			return
		}

		// Extraire la cat√©gorie de base de l'URL actuelle
		baseCategory := e.Request.URL.Path
		if strings.Contains(baseCategory, "?") {
			baseCategory = strings.Split(baseCategory, "?")[0]
		}

		mutex.Lock()
		pagesVisited := visitedPages[baseCategory]
		mutex.Unlock()

		if pagesVisited < maxPages {
			mutex.Lock()
			visitedPages[baseCategory] = pagesVisited + 1
			mutex.Unlock()

			// Log de pagination supprim√© pour r√©duire la verbosit√©

			// Visiter la page suivante avec un d√©lai al√©atoire plus long
			randomDelay := getRandomDelay(2000, 5000) // D√©lai al√©atoire entre 2s et 5s
			time.Sleep(randomDelay)
			collector.Visit(nextPageURL)
		} else {
			log.Printf("‚úÖ Limite de pages atteinte pour %s (%d pages)\n", baseCategory, maxPages)
		}
	})

	return collector
}

// createRecipeCollector cr√©e un collecteur pour collecter une recette individuelle
func createRecipeCollector(stats *ScrapingStats) *colly.Collector {
	collector := colly.NewCollector()

	// Configuration avec d√©lais plus longs pour √©viter la d√©tection
	collector.Limit(&colly.LimitRule{
		DomainGlob:  "*",
		Parallelism: 1,
		Delay:       2 * time.Second, // D√©lai de base augment√© √† 2 secondes
	})

	collector.OnRequest(func(r *colly.Request) {
		// Configurer les headers r√©alistes pour √©viter la d√©tection
		configureRealisticHeaders(r)

		// Les d√©lais al√©atoires sont g√©r√©s automatiquement par Colly via RandomDelay dans LimitRule
		stats.IncrementRecipeRequest()
		// Log de requ√™te supprim√© pour r√©duire la verbosit√©
	})

	// G√©rer les erreurs HTTP (403, 429, etc.)
	collector.OnError(func(r *colly.Response, err error) {
		statusCode := r.StatusCode
		if statusCode == 403 || statusCode == 429 {
			log.Printf("‚ö†Ô∏è  Erreur %d d√©tect√©e pour la recette %s: %v\n", statusCode, r.Request.URL, err)
			log.Printf("üîÑ Attente prolong√©e avant retry (10-20s)...\n")
			// Attendre beaucoup plus longtemps en cas d'erreur (10-20 secondes)
			time.Sleep(getRandomDelay(10000, 20000))
		} else {
			log.Printf("‚ùå Erreur HTTP %d pour la recette %s: %v\n", statusCode, r.Request.URL, err)
		}
	})

	return collector
}

// scrapeRecipeDetails configure les handlers pour collecter les d√©tails d'une recette
func scrapeRecipeDetails(collector *colly.Collector, recipe *Recipe, completedRecipes chan<- Recipe, stats *ScrapingStats) {
	// Collecter les ingr√©dients - Nouveaux s√©lecteurs CSS pour AllRecipes 2024
	collector.OnHTML("ul.mm-recipes-structured-ingredients__list", func(e *colly.HTMLElement) {
		var ingredients []Ingredient

		e.ForEach("li.mm-recipes-structured-ingredients__list-item", func(_ int, ingr *colly.HTMLElement) {
			// Extraire la quantit√© et l'unit√© s√©par√©ment
			quantity := strings.TrimSpace(ingr.ChildText("span[data-ingredient-quantity=true]"))
			unit := strings.TrimSpace(ingr.ChildText("span[data-ingredient-unit=true]"))
			name := strings.TrimSpace(ingr.ChildText("span[data-ingredient-name=true]"))

			// Si on a des donn√©es structur√©es, les utiliser
			if quantity != "" || unit != "" || name != "" {
				// Construire le texte complet de l'ingr√©dient
				fullText := strings.TrimSpace(ingr.Text)
				ingredients = append(ingredients, Ingredient{
					Quantity: fullText, // Texte complet pour l'instant
					Unit:     "",       // Pas de s√©paration pour l'instant
				})
			}
		})

		recipe.Ingredients = ingredients
		// Log d'ingr√©dients supprim√© pour r√©duire la verbosit√©
	})

	// Collecter les instructions - Nouveaux s√©lecteurs CSS pour AllRecipes 2024
	collector.OnHTML("div.mm-recipes-steps__content", func(e *colly.HTMLElement) {
		var instructions []Instruction

		// Chercher dans les listes ordonn√©es avec la structure correcte
		e.ForEach("ol.mntl-sc-block li", func(i int, inst *colly.HTMLElement) {
			number := strconv.Itoa(i + 1)
			// Extraire le texte de la balise <p> √† l'int√©rieur du <li>
			description := strings.TrimSpace(inst.ChildText("p.mntl-sc-block-html"))
			if description == "" {
				// Fallback sur le texte complet si pas de balise p
				description = strings.TrimSpace(inst.Text)
			}
			if description != "" {
				instructions = append(instructions, Instruction{
					Number:      number,
					Description: description,
				})
			}
		})

		recipe.Instructions = instructions
		// Log d'instructions supprim√© pour r√©duire la verbosit√©
	})

	// Quand la collecte de la recette est termin√©e
	collector.OnScraped(func(r *colly.Response) {
		stats.IncrementRecipesCompleted()
		completedRecipes <- *recipe
		// Log de recette compl√©t√©e supprim√© pour r√©duire la verbosit√© (trop de logs)
	})
}

// processRecipeReusable traite une recette dans un worker r√©utilisable
func processRecipeReusable(recipeData RecipeData, stats *ScrapingStats, completedRecipes chan<- Recipe, workerStats *WorkerStats) {
	startTime := time.Now()
	// Log de traitement supprim√© pour r√©duire la verbosit√©

	// Cr√©er un collecteur d√©di√© pour cette recette
	recipeCollector := createRecipeCollector(stats)

	recipe := Recipe{
		Name:  recipeData.Title,
		Page:  recipeData.URL,
		Image: recipeData.Image,
	}

	// Configurer la collecte des d√©tails
	scrapeRecipeDetails(recipeCollector, &recipe, completedRecipes, stats)

	// Visiter la page de la recette
	err := recipeCollector.Visit(recipeData.URL)
	if err != nil {
		stats.IncrementRecipesFailed()
		log.Printf("‚ùå Worker #%d - Erreur lors de la visite de la page de recette '%s': %v\n", workerStats.WorkerID, recipeData.Title, err)
	} else {
		// Mettre √† jour les stats du worker
		workerStats.RequestsHandled++
		workerStats.RecipesProcessed++
	}

	duration := time.Since(startTime)
	_ = duration // Utilis√© pour les stats mais pas logg√© pour r√©duire la verbosit√©
}

// startRecipeProcessor d√©marre la goroutine qui traite les URLs de recettes
func startRecipeProcessor(recipeURLs <-chan RecipeData, completedRecipes chan<- Recipe, stats *ScrapingStats, wg *sync.WaitGroup) {
	go func() {
		maxWorkers := stats.MaxWorkers // Utiliser le nombre optimal calcul√© automatiquement
		semaphore := make(chan struct{}, maxWorkers)

		// Log d'initialisation supprim√© pour r√©duire la verbosit√©

		// Cr√©er des workers r√©utilisables
		for i := 0; i < maxWorkers; i++ {
			wg.Add(1)
			go func(workerID int) {
				defer wg.Done()
				workerStats := WorkerStats{
					WorkerID:         workerID,
					RequestsHandled:  0,
					RecipesProcessed: 0,
					StartTime:        time.Now(),
				}

				// Log de d√©marrage worker supprim√© pour r√©duire la verbosit√©

				// Le worker traite les recettes en continu
				for recipeData := range recipeURLs {
					// Acqu√©rir un slot dans le semaphore
					semaphore <- struct{}{}

					// Traiter la recette
					processRecipeReusable(recipeData, stats, completedRecipes, &workerStats)

					// Lib√©rer le slot
					<-semaphore
				}

				// Mettre √† jour les stats finales du worker
				workerStats.EndTime = time.Now()
				workerStats.Duration = workerStats.EndTime.Sub(workerStats.StartTime)
				stats.Mutex.Lock()
				stats.WorkerStats[workerID] = workerStats
				stats.Mutex.Unlock()

				// Log de fin worker supprim√© pour r√©duire la verbosit√©
			}(i)
		}

		// Log de workers d√©marr√©s supprim√© pour r√©duire la verbosit√©

		// Attendre que toutes les goroutines se terminent
		wg.Wait()
		close(completedRecipes)
		// Log de fin workers supprim√© pour r√©duire la verbosit√©
	}()
}

// startRecipeCollector d√©marre la goroutine qui collecte les recettes termin√©es
func startRecipeCollector(completedRecipes <-chan Recipe, recipes *[]Recipe, recipesMutex *sync.RWMutex, done chan<- bool) {
	go func() {
		for recipe := range completedRecipes {
			recipesMutex.Lock()
			*recipes = append(*recipes, recipe)
			recipesMutex.Unlock()
		}
		done <- true
	}()
}

// saveRecipesToFile sauvegarde les recettes dans un fichier JSON
func saveRecipesToFile(recipes []Recipe, filename string) error {
	content, err := json.MarshalIndent(recipes, "", "  ")
	if err != nil {
		return err
	}

	return os.WriteFile(filename, content, 0644)
}

// printDetailedStats affiche les statistiques d√©taill√©es
func printDetailedStats(stats *ScrapingStats, filename string) {
	stats.CalculateFinalStats()
	detailedStats := stats.GetDetailedStats()

	fmt.Println("\n" + strings.Repeat("=", 80))
	fmt.Println("üìä STATISTIQUES D√âTAILL√âES DU COLLECTEUR")
	fmt.Println(strings.Repeat("=", 80))

	// Performance g√©n√©rale
	fmt.Printf("‚è±Ô∏è  Dur√©e totale: %v\n", detailedStats.TotalDuration)
	fmt.Printf("üöÄ Requ√™tes par seconde: %.2f\n", detailedStats.RequestsPerSecond)
	fmt.Printf("üìù Recettes par seconde: %.2f\n", detailedStats.RecipesPerSecond)

	// Requ√™tes
	fmt.Println("\nüåê REQU√äTES:")
	fmt.Printf("   Total: %d\n", detailedStats.TotalRequests)
	fmt.Printf("   Page principale: %d\n", detailedStats.MainPageRequests)
	fmt.Printf("   Pages recettes: %d\n", detailedStats.RecipeRequests)

	// Recettes
	fmt.Println("\nüìù RECETTES:")
	fmt.Printf("   Trouv√©es: %d\n", detailedStats.RecipesFound)
	fmt.Printf("   Compl√©t√©es: %d\n", detailedStats.RecipesCompleted)
	fmt.Printf("   √âchou√©es: %d\n", detailedStats.RecipesFailed)
	fmt.Printf("   Taux de succ√®s: %.1f%%\n", float64(detailedStats.RecipesCompleted)/float64(detailedStats.RecipesFound)*100)

	// Configuration automatique
	numLogicalCPU := runtime.NumCPU()
	numPhysicalCores := getPhysicalCores()
	adaptiveRatio := calculateAdaptiveRatio(numPhysicalCores)
	fmt.Println("\nüíª CONFIGURATION AUTOMATIQUE:")
	fmt.Printf("   Processeurs logiques: %d\n", numLogicalCPU)
	fmt.Printf("   C≈ìurs physiques d√©tect√©s: %d\n", numPhysicalCores)
	fmt.Printf("   Ratio adaptatif: %d (calcul√© automatiquement)\n", adaptiveRatio)
	fmt.Printf("   Calcul: %d c≈ìurs √ó %d = %d workers\n", numPhysicalCores, adaptiveRatio, numPhysicalCores*adaptiveRatio)
	fmt.Printf("   Configuration finale: %d workers\n", detailedStats.MaxWorkers)

	// D√©tails par worker
	if len(detailedStats.WorkerStats) > 0 {
		fmt.Println("\nüìà PERFORMANCE PAR WORKER:")
		for workerID, workerStats := range detailedStats.WorkerStats {
			fmt.Printf("   Worker #%d: %d requ√™tes, %d recettes, %v\n",
				workerID, workerStats.RequestsHandled, workerStats.RecipesProcessed, workerStats.Duration)
		}
	}

	// Calculs de performance
	avgRequestsPerRecipe := float64(detailedStats.RecipeRequests) / float64(detailedStats.RecipesCompleted)
	fmt.Println("\nüí° ANALYSE DE PERFORMANCE:")
	fmt.Printf("   Requ√™tes moyennes par recette: %.1f\n", avgRequestsPerRecipe)
	fmt.Printf("   D√©bit estim√©: %.0f requ√™tes/seconde\n", detailedStats.RequestsPerSecond)

	if detailedStats.RecipesPerSecond > 0 {
		fmt.Printf("   Temps moyen par recette: %.2f secondes\n", 1/detailedStats.RecipesPerSecond)
	}

	fmt.Printf("\nüíæ Fichier de sortie: %s\n", filename)
	fmt.Println(strings.Repeat("=", 80))
}

// printRealTimeStats affiche les statistiques en temps r√©el (d√©sactiv√© pour r√©duire la verbosit√©)
func printRealTimeStats(stats *ScrapingStats) {
	// Logs de temps r√©el d√©sactiv√©s pour r√©duire la verbosit√©
	// Les statistiques finales sont toujours affich√©es √† la fin
}

// main est la fonction principale du collecteur
// Elle orchestre tout le processus de collecte : collecte des URLs, traitement des recettes, et sauvegarde
func main() {
	// ===== PHASE 1: INITIALISATION =====
	// Afficher les informations de version et de build
	printVersionInfo()

	// Configuration du collecteur - param√®tres ajustables
	const minWorkers = 1          // Nombre minimum de workers
	const maxWorkers = 100        // Nombre maximum de workers
	const maxPagesPerCategory = 5 // Nombre maximum de pages √† collecter par cat√©gorie
	const maxRecipesPerPage = 20  // Estimation du nombre de recettes par page

	// Configuration automatique bas√©e sur les ressources CPU
	optimalWorkers := calculateOptimalWorkers(minWorkers, maxWorkers)

	// Configuration automatique (logs supprim√©s pour r√©duire la verbosit√©)

	// Cr√©er l'objet de statistiques thread-safe
	stats := NewScrapingStats(optimalWorkers)

	// Note: Dans Go 1.20+, le g√©n√©rateur global rand est automatiquement initialis√©
	// Pas besoin d'appeler rand.Seed() qui est d√©pr√©ci√©

	// Afficher les informations de d√©marrage (simplifi√©)
	log.Printf("üöÄ Collecteur d√©marr√© avec %d workers\n", optimalWorkers)
	log.Printf("üìä Configuration: %d pages/cat√©gorie, %d recettes/page max\n", maxPagesPerCategory, maxRecipesPerPage)

	// D√©marrer l'affichage des statistiques en temps r√©el (d√©sactiv√© pour r√©duire la verbosit√©)
	printRealTimeStats(stats)

	// ===== PHASE 2: CONFIGURATION DES CHANNELS =====
	// Channels pour la communication entre goroutines (pipeline de donn√©es)
	recipeURLs := make(chan RecipeData, 2000)   // Channel pour les URLs de recettes (buffer de 2000)
	completedRecipes := make(chan Recipe, 2000) // Channel pour les recettes compl√©t√©es (buffer de 2000)
	done := make(chan bool)                     // Channel de signalisation de fin

	// Slice thread-safe pour stocker toutes les recettes finales
	var recipes []Recipe
	var recipesMutex sync.RWMutex // Mutex pour prot√©ger l'acc√®s concurrent au slice

	// WaitGroup pour synchroniser l'attente de la fin de toutes les goroutines
	var wg sync.WaitGroup

	// ===== PHASE 3: CONFIGURATION DES COLLECTEURS =====
	// Cr√©er le collecteur principal avec support de la pagination
	mainCollector := createMainCollectorWithPagination(stats, recipeURLs, maxPagesPerCategory)

	// ===== PHASE 4: D√âMARRAGE DES GOROUTINES DE TRAITEMENT =====
	// D√©marrer la goroutine qui collecte les recettes termin√©es
	startRecipeCollector(completedRecipes, &recipes, &recipesMutex, done)

	// D√©marrer les workers qui traitent les URLs de recettes
	startRecipeProcessor(recipeURLs, completedRecipes, stats, &wg)

	// ===== PHASE 5: D√âFINITION DES CAT√âGORIES √Ä COLLECTER =====
	// Liste des cat√©gories de recettes AllRecipes √† collecter
	// Chaque cat√©gorie sera visit√©e avec pagination automatique
	categories := []string{
		"https://www.allrecipes.com/recipes/16369/soups-stews-and-chili/soup/",               // Soupes
		"https://www.allrecipes.com/recipes/1246/soups-stews-and-chili/soup/chicken-soup/",   // Soupes de poulet
		"https://www.allrecipes.com/recipes/76/appetizers-and-snacks/",                       // Ap√©ritifs et collations
		"https://www.allrecipes.com/recipes/113/appetizers-and-snacks/pastries/",             // P√¢tisseries
		"https://www.allrecipes.com/recipes/1059/fruits-and-vegetables/vegetables/",          // L√©gumes
		"https://www.allrecipes.com/recipes/1083/fruits-and-vegetables/vegetables/cucumber/", // Concombres
		"https://www.allrecipes.com/recipes/77/drinks/",                                      // Boissons
		"https://www.allrecipes.com/recipes/79/desserts/",                                    // Desserts
		"https://www.allrecipes.com/recipes/81/side-dish/",                                   // Accompagnements
		"https://www.allrecipes.com/recipes/1569/everyday-cooking/on-the-go/tailgating/",     // Tailgating
	}

	// ===== PHASE 6: VISITE INITIALE DE LA PAGE D'ACCUEIL =====
	// Visiter la page d'accueil pour obtenir les cookies de session (important pour contourner Cloudflare)
	log.Printf("Visite de la page d'accueil pour obtenir les cookies de session...\n")
	homepageCollector := colly.NewCollector()
	homepageCollector.OnRequest(func(r *colly.Request) {
		configureRealisticHeaders(r)
		// Pour la premi√®re visite, utiliser Google comme referer
		r.Headers.Set("Referer", "https://www.google.com/")
	})
	err := homepageCollector.Visit("https://www.allrecipes.com/")
	if err != nil {
		log.Printf("‚ö†Ô∏è  Erreur lors de la visite de la page d'accueil: %v\n", err)
	} else {
		log.Printf("‚úÖ Page d'accueil visit√©e avec succ√®s, cookies de session obtenus\n")
		// Attendre un peu apr√®s la visite de la page d'accueil
		time.Sleep(getRandomDelay(2000, 4000))
	}

	// ===== PHASE 7: EX√âCUTION DE LA COLLECTE =====
	// D√©marrer la collecte de toutes les cat√©gories d√©finies
	log.Printf("D√©but de la collecte de %d cat√©gories...\n", len(categories))
	for i, category := range categories {
		// Log de cat√©gorie supprim√© pour r√©duire la verbosit√©
		_ = i // Variable utilis√©e mais pas logg√©e

		// Visiter la cat√©gorie (avec pagination automatique)
		err := mainCollector.Visit(category)
		if err != nil {
			log.Printf("‚ö†Ô∏è  Erreur lors de la visite de la cat√©gorie %s: %v\n", category, err)
			continue // Continuer avec la cat√©gorie suivante en cas d'erreur
		}

		// Pause respectueuse entre les cat√©gories avec d√©lai al√©atoire augment√©
		randomDelay := getRandomDelay(5000, 10000) // D√©lai al√©atoire entre 5s et 10s
		time.Sleep(randomDelay)
	}

	// ===== PHASE 8: FINALISATION =====
	// Fermer le channel des URLs pour signaler qu'il n'y a plus de recettes √† traiter
	close(recipeURLs)

	// Attendre que toutes les recettes soient collect√©es (signal du collector)
	<-done

	// ===== PHASE 9: SAUVEGARDE ET STATISTIQUES =====
	// Sauvegarder toutes les recettes dans un fichier JSON
	filename := "data.json"
	recipesMutex.RLock()
	err = saveRecipesToFile(recipes, filename)
	recipesMutex.RUnlock()

	if err != nil {
		log.Printf("Erreur lors de l'enregistrement des recettes: %v\n", err)
		return
	}

	// Afficher les statistiques d√©taill√©es de performance
	printDetailedStats(stats, filename)

	// Afficher les informations de build dans les logs finaux
	log.Printf("‚úÖ Collecte termin√©e\n")
}
